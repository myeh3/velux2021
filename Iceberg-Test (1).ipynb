{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1037525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob\n",
    "#connection string:#AccountName=dyvenia1;AccountKey=20+CEDaPAJ5Hs3wnzzd+s+4S5lzzsS+Y4s8ocbs+ZOyj5jlM7TzcAS+WlLQ3nZK7Oh/hakziWkahkPX8BaSYBw==;EndpointSuffix=core.windows.net;DefaultEndpointsProtocol=https;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6bd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "container = ContainerClient.from_connection_string(conn_str=\"AccountName=dyvenia1;AccountKey=20+CEDaPAJ5Hs3wnzzd+s+4S5lzzsS+Y4s8ocbs+ZOyj5jlM7TzcAS+WlLQ3nZK7Oh/hakziWkahkPX8BaSYBw==;EndpointSuffix=core.windows.net;DefaultEndpointsProtocol=https;\", container_name=\"tests\")\n",
    "filelist=[]\n",
    "blob_list = container.list_blobs()\n",
    "for file in blob_list:\n",
    "    if file.name.startswith('kafka/c4c/') and file.name.endswith('.json'):\n",
    "        filelist.append(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482184dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%unset SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c219ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a0f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6480fcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.lang.IllegalArgumentException: basedir must be absolute: ?/.ivy2/local\n",
      "\tat org.apache.ivy.util.Checks.checkAbsolute(Checks.java:48)\n",
      "\tat org.apache.ivy.plugins.repository.file.FileRepository.setBaseDir(FileRepository.java:131)\n",
      "\tat org.apache.ivy.plugins.repository.file.FileRepository.<init>(FileRepository.java:44)\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers(SparkSubmit.scala:1179)\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.buildIvySettings(SparkSubmit.scala:1281)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:182)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:308)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#https://github.com/apache/iceberg/issues/4424\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sparkSession \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 339\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "#https://github.com/apache/iceberg/issues/4424\n",
    "sparkSession = SparkSession.builder.appName(\"test\").getOrCreate()'''\\\n",
    ".config('spark.jars.packages', 'iceberg-spark-runtime-3.2_2.12:0.13.1')\\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    ".config('spark.sql.catalog.spark_catalog', 'org.apache.iceberg.spark.SparkSessionCatalog')\\\n",
    ".config('spark.sql.catalog.spark_catalog.type', 'hive')\\\n",
    ".config('spark.sql.catalog.local', 'org.apache.iceberg.spark.SparkCatalog')\\\n",
    ".config('spark.sql.catalog.local.type', 'hadoop')\\\n",
    ".config('spark.sql.catalog.local.warehouse', '$PWD/warehouse')\\\n",
    ".config(\"hive.metastore.uris\", \"thrift://hive:9083\")\\\n",
    ".config(\"spark.sql.catalog.hive_prod\",\"org.apache.iceberg.spark.SparkCatalog\")\\\n",
    ".config(\"fs.azure.account.key.dyvenia1.blob.core.windows.net\",\"20+CEDaPAJ5Hs3wnzzd+s+4S5lzzsS+Y4s8ocbs+ZOyj5jlM7TzcAS+WlLQ3nZK7Oh/hakziWkahkPX8BaSYBw==\")\\\n",
    ".config(\"spark.sql.catalog.hive_prod.type\",\"hive\")\\\n",
    ".config(\"spark.sql.catalog.hive_prod.uri\",\"thrift://hive:9083\")\\\n",
    ".config('hive.metastore.warehouse.dir', 's3a://veluxusf/hive')\\\n",
    ".config('spark.sql.catalog.hive_prod.warehouse', 's3a://veluxusf/hive')\\\n",
    ".getOrCreate()'''\n",
    "#.config(\"spark.sql.catalog.hive_prod\", \"org.apache.iceberg.spark.SparkCatalog\")\\\n",
    "#.config(\"spark.sql.catalog.hive_prod.type\", \"hive\")\\\n",
    "#.config(\"spark.sql.catalog.hive_prod.uri\", \"thrift://hive:9083\")\\\n",
    "#.enableHiveSupport()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b68526",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sparkSession.read.json(\n",
    "    f\"wasbs://tests@dyvenia1.blob.core.windows.net/{filelist[0]}\")\n",
    "\n",
    "for file in filelist[1:10]:\n",
    "    try:\n",
    "        sdf=sdf.union(sparkSession.read.json(f\"wasbs://tests@dyvenia1.blob.core.windows.net/{file}\"))\n",
    "    except:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df15c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"sdf3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE hive_prod.default.sdf9\n",
    "USING iceberg AS SELECT * FROM hive_prod.default.sdf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c5a6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from hive_prod.default.sdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef905162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb05adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce69b4ab",
   "metadata": {},
   "source": [
    "# Hiveberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show databases;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "use test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c5c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE or REPLACE TABLE  expp using hive; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826fd920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM exp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1af7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table ice;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table ice;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11276a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"tempview\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE or REPLACE TABLE test.sdf USING iceberg AS SELECT * FROM tempview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ea374",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from test.sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare data lake path where we want to write the data\n",
    "\n",
    "#write as parquet data\n",
    "sdf.write.format(\"iceberg\").save('wasbs://tests@dyvenia1.blob.core.windows.net/kafka/iceberg/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dba99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=sparkSession.read.json(f\"wasbs://tests@dyvenia1.blob.core.windows.net/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select(\"BUYERPARTYID\",\"CREATIONDATETIME\").write.format(\"iceberg\").save('wasbs://tests@dyvenia1.blob.core.windows.net/kafka/iceberg/test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65c05e",
   "metadata": {},
   "source": [
    "# Spark Local Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%sql\n",
    "df_test.write.format(\"iceberg\").save(\"db.table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a6274",
   "metadata": {},
   "source": [
    "# Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from iceberg.hive import HiveTables\n",
    "\n",
    "# instantiate Hive Tables\n",
    "#conf = {\"hive.metastore.uris\": 'thrift://hive:9083'}\n",
    "#tables = HiveTables(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0de350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
